import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import argparse

def validate_paths(base_model_path, lora_path):
    # æ£€æŸ¥åŸºç¡€æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨å¿…è¦æ–‡ä»¶
    required_files = ["config.json", "tokenizer.model"]
    for f in required_files:
        if not os.path.exists(os.path.join(base_model_path, f)):
            raise FileNotFoundError(f"âŒ ç¼ºå¤±åŸºç¡€æ¨¡å‹æ–‡ä»¶: {f} in {base_model_path}")

    # æ£€æŸ¥ LoRA æƒé‡è·¯å¾„
    lora_files = ["adapter_model.bin", "adapter_config.json"]
    for f in lora_files:
        if not os.path.exists(os.path.join(lora_path, f)):
            raise FileNotFoundError(f"âŒ ç¼ºå¤± LoRA æ–‡ä»¶: {f} in {lora_path}")

def load_model(base_model_path, lora_path):
    print("ğŸš€ æ­£åœ¨åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    tokenizer.pad_token = tokenizer.eos_token

    print("ğŸ“¦ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )

    print("ğŸ”§ æ­£åœ¨åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(base_model, lora_path)

    print("ğŸ§© åˆå¹¶ LoRA åˆ°ä¸»æ¨¡å‹ä¸­ï¼ˆæå‡æ¨ç†é€Ÿåº¦ï¼‰...")
    model = model.merge_and_unload()

    return model, tokenizer

def chat_loop(model, tokenizer, max_new_tokens=256, temperature=0.7):
    print("\nğŸ©º æ¬¢è¿ä½¿ç”¨åŒ»å­¦å¤§æ¨¡å‹é—®ç­”ç³»ç»Ÿï¼è¾“å…¥ 'exit' å¯é€€å‡ºå¯¹è¯ã€‚\n")
    while True:
        user_input = input("ğŸ‘¤ ä½ ï¼š")
        if user_input.lower() in ["exit", "quit", "q"]:
            print("ğŸ‘‹ å·²é€€å‡ºã€‚")
            break

        inputs = tokenizer(user_input, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        reply = response[len(user_input):].strip()
        print("ğŸ¤– æ¨¡å‹ï¼š", reply)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åŒ»å­¦ LoRA èŠå¤©åŠ©æ‰‹")
    parser.add_argument("--base_model", type=str, default="./checkpoints/llama-7b-hf")
    parser.add_argument("--lora_path", type=str, default="./checkpoints/lora-llama-med")
    args = parser.parse_args()

    validate_paths(args.base_model, args.lora_path)
    model, tokenizer = load_model(args.base_model, args.lora_path)
    chat_loop(model, tokenizer)
